{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f89267",
   "metadata": {},
   "source": [
    "### 한일 기후변화비교 연구\n",
    "#### Step 2. 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5704a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Long format (파일별) ===\n",
      "     media  year  article_count                             file\n",
      "조선일보  2022            339 조선일보_2022년도 데이터.xlsx\n",
      "조선일보  2023            402 조선일보_2023년도 데이터.xlsx\n",
      "조선일보  2024            299 조선일보_2024년도 데이터.xlsx\n",
      "   한겨레  2022            732    한겨레_2022년도 데이터.xlsx\n",
      "   한겨레  2023            569    한겨레_2023년도 데이터.xlsx\n",
      "   한겨레  2024            557    한겨레_2024년도 데이터.xlsx\n",
      "\n",
      "=== Wide format (언론사 x 연도) ===\n",
      "year        2022  2023  2024\n",
      "media                       \n",
      "조선일보   339   402   299\n",
      "한겨레      732   569   557\n"
     ]
    }
   ],
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import openpyxl\n",
    "\n",
    "# BASE_DIR = \"E:/Data_for_Practice/JapMedia/\"\n",
    "BASE_DIR = '/Users/keungouikim/Library/CloudStorage/GoogleDrive-awekimm@gmail.com/내 드라이브/[YU]/[Research]/24_JWKIM/'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data/kor_data\")\n",
    "\n",
    "pattern = re.compile(r'^(?P<media>.+?)_(?P<year>20\\d{2})')\n",
    "\n",
    "records = []\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    if not fname.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        continue\n",
    "    \n",
    "    m = pattern.search(fname)\n",
    "    if m is None:\n",
    "        continue\n",
    "    \n",
    "    media = m.group(\"media\")\n",
    "    year = int(m.group(\"year\"))\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "\n",
    "    df = pd.read_excel(fpath)  \n",
    "    article_count = len(df)\n",
    "\n",
    "    records.append({\n",
    "        \"media\": media,\n",
    "        \"year\": year,\n",
    "        \"file\": fname,\n",
    "        \"article_count\": article_count\n",
    "    })\n",
    "\n",
    "counts_long = pd.DataFrame(records).sort_values([\"media\", \"year\"])\n",
    "\n",
    "counts_wide = counts_long.pivot_table(\n",
    "    index=\"media\",\n",
    "    columns=\"year\",\n",
    "    values=\"article_count\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ").astype(int)\n",
    "\n",
    "print(\"=== Long format (파일별) ===\")\n",
    "print(counts_long[[\"media\", \"year\", \"article_count\", \"file\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Wide format (언론사 x 연도) ===\")\n",
    "print(counts_wide.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00808d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\rtx5090\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "\n",
      "===== Processing 조선일보_2022년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 조선일보_2022년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9276\n",
      "\n",
      "===== Processing 조선일보_2023년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 조선일보_2023년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9148\n",
      "\n",
      "===== Processing 조선일보_2024년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 조선일보_2024년도 데이터_translated_gpt.csv | Topics: 3 | Score: 0.9272\n",
      "\n",
      "===== Processing 한겨레_2022년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6/6 [00:04<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 한겨레_2022년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9298\n",
      "\n",
      "===== Processing 한겨레_2023년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 한겨레_2023년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.8858\n",
      "\n",
      "===== Processing 한겨레_2024년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 한겨레_2024년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9313\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "BASE_DIR = \"E:/Data_for_Practice/JapMedia/\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data/kor_data\")\n",
    "EMB_DIR = os.path.join(BASE_DIR, \"data/embed_figure\")\n",
    "TEXT_COL = \"본문_en\"\n",
    "\n",
    "SEED = 42\n",
    "EMBEDDING_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "MIN_TOKENS_AFTER_CLEAN = 10\n",
    "NR_TOPICS_FIXED = 5\n",
    "\n",
    "MIN_DF_FIXED = 0.01\n",
    "MAX_DF_FIXED = 0.90\n",
    "\n",
    "TOPN_WORDS_FOR_DIVERSITY = 10\n",
    "ALPHA_DIVERSITY = 0.6\n",
    "BETA_COHESION = 0.4\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device \n",
    "def pick_device_for_st():\n",
    "    try:\n",
    "        import torch\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = pick_device_for_st()\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "# Stopwords \n",
    "NEWS_STOPWORDS_EXTRA = {\n",
    "    \"said\",\"will\",\"one\",\"two\",\"new\",\"year\",\"years\",\"percent\",\"also\",\"could\",\"would\",\n",
    "    \"mr\",\"ms\",\"u\",\"korea\",\"south\",\"seoul\",\"korean\",\"however\",\"among\",\"may\",\"many\",\n",
    "    \"made\",\"make\",\"like\",\"since\",\"according\",\"including\",\"told\",\"say\",\"says\",\n",
    "    \"first\",\"last\",\"day\",\"days\",\"week\",\"weeks\",\"month\",\"months\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"time\",\"times\",\"report\",\"reports\",\"reported\",\"yonhap\",\"reuters\",\"ap\",\n",
    "    \"people\",\"company\",\"companies\",\"government\",\"official\",\"officials\",\n",
    "    \"article\",\"news\",\"daily\",\"kim\",\"park\",\"lee\",\"cho\",\"jang\",\"chung\",\"moon\",\"yoon\"\n",
    "}\n",
    "all_stop_words = set(ENGLISH_STOP_WORDS).union(NEWS_STOPWORDS_EXTRA)\n",
    "\n",
    "# Preprocessing \n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "except Exception:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def clean_text_lemma(t: str, stopwords: set) -> str:\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" \", t)\n",
    "    t = re.sub(r\"\\b\\d+\\b\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z\\s\\-]\", \" \", t)\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    doc = nlp(t)\n",
    "    lemmas = [\n",
    "        token.lemma_ for token in doc\n",
    "        if len(token.lemma_) > 2 and token.lemma_ not in stopwords and token.is_alpha\n",
    "    ]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Embedding \n",
    "embedding_model = SentenceTransformer(EMBEDDING_NAME, device=DEVICE)\n",
    "def encode_texts(texts):\n",
    "    return embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=48 if DEVICE == \"cpu\" else 128,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "# Scoring\n",
    "def topic_diversity(topic_model, topn=TOPN_WORDS_FOR_DIVERSITY):\n",
    "    info = topic_model.get_topic_info()\n",
    "    topic_ids = [t for t in info[\"Topic\"].tolist() if t != -1]\n",
    "    words = []\n",
    "    for t in topic_ids:\n",
    "        reps = topic_model.get_topic(t)\n",
    "        if reps:\n",
    "            words.extend([w for (w, _) in reps[:topn]])\n",
    "    return len(set(words)) / (len(set(topic_ids)) * topn) if topic_ids else 0.0\n",
    "\n",
    "def topic_cohesion(embeddings, labels):\n",
    "    labels = np.array(labels)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() < 2:\n",
    "        return 0.0\n",
    "    emb = embeddings[mask]\n",
    "    lab = labels[mask]\n",
    "    sims = []\n",
    "    for t in np.unique(lab):\n",
    "        idx = np.where(lab == t)[0]\n",
    "        if len(idx) > 1:\n",
    "            sub = emb[idx]\n",
    "            centroid = np.mean(sub, axis=0, keepdims=True)\n",
    "            sims.extend(cosine_similarity(sub, centroid).ravel())\n",
    "    return np.mean(sims) if sims else 0.0\n",
    "\n",
    "def composite_score(model, emb, labels, alpha=ALPHA_DIVERSITY, beta=BETA_COHESION):\n",
    "    td = topic_diversity(model)\n",
    "    coh = topic_cohesion(emb, labels)\n",
    "    return alpha * td + beta * coh, td, coh\n",
    "\n",
    "# Fit function \n",
    "def fit_evaluate(params, docs, emb):\n",
    "    if len(docs) < 5:\n",
    "        return -9999, 0, 0, 0, None\n",
    "\n",
    "    vec = CountVectorizer(stop_words=list(all_stop_words),\n",
    "                          ngram_range=params[\"n_gram_range\"],\n",
    "                          min_df=MIN_DF_FIXED, max_df=MAX_DF_FIXED)\n",
    "\n",
    "    umap_m = UMAP(\n",
    "        n_neighbors=params[\"n_neighbors\"],\n",
    "        min_dist=params[\"min_dist\"],\n",
    "        n_components=10,\n",
    "        metric=\"cosine\",\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    hdb = HDBSCAN(\n",
    "        min_cluster_size=params[\"min_cluster_size\"],\n",
    "        min_samples=5,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    ctfidf = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "    model = BERTopic(\n",
    "        n_gram_range=params[\"n_gram_range\"],\n",
    "        nr_topics=NR_TOPICS_FIXED,\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vec,\n",
    "        ctfidf_model=ctfidf,\n",
    "        umap_model=umap_m,\n",
    "        hdbscan_model=hdb,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    topics, _ = model.fit_transform(docs, emb)\n",
    "    score, td, coh = composite_score(model, emb, topics)\n",
    "    n_topics = len([t for t in model.get_topic_info()[\"Topic\"] if t != -1])\n",
    "\n",
    "    return score, td, coh, n_topics, model\n",
    "\n",
    "# Per-file pipeline \n",
    "def process_file(file_path: str):\n",
    "    fname = os.path.basename(file_path)\n",
    "    print(f\"\\n===== Processing {fname} =====\")\n",
    "\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    if TEXT_COL not in df.columns:\n",
    "        print(f\"[SKIP] No '{TEXT_COL}' column\")\n",
    "        return\n",
    "\n",
    "    # newspaper / year parsing from filename \n",
    "    base_no_ext = os.path.splitext(fname)[0]\n",
    "\n",
    "    m = re.search(r\"^([^_]+)_(\\d{4})년도\", base_no_ext)\n",
    "    if m:\n",
    "        newspaper = m.group(1)\n",
    "        year = int(m.group(2))\n",
    "    else:\n",
    "        newspaper = \"UNKNOWN\"\n",
    "        year = None\n",
    "        print(f\"[WARN] Could not parse newspaper/year from filename: {fname}\")\n",
    "\n",
    "    # preprocessing \n",
    "    df = df[df[TEXT_COL].notna()].copy()\n",
    "    df[\"clean_text\"] = df[TEXT_COL].map(lambda x: clean_text_lemma(x, all_stop_words))\n",
    "    df = df[df[\"clean_text\"].str.split().map(len) >= MIN_TOKENS_AFTER_CLEAN]\n",
    "    docs = df[\"clean_text\"].tolist()\n",
    "    if len(docs) < 5:\n",
    "        print(f\"[SKIP] Too few docs ({len(docs)})\")\n",
    "        return\n",
    "\n",
    "    # embedding \n",
    "    emb = encode_texts(docs)\n",
    "\n",
    "    # topic modeling \n",
    "    best_param = {\"n_gram_range\": (1,1), \"n_neighbors\": 10, \"min_dist\": 0.1, \"min_cluster_size\": 25}\n",
    "    score, td, coh, n_topics, model = fit_evaluate(best_param, docs, emb)\n",
    "    print(f\"[DONE] {fname} | Topics: {n_topics} | Score: {score:.4f}\")\n",
    "\n",
    "    topics, probs = model.fit_transform(docs, emb)\n",
    "\n",
    "    df[\"topic\"] = topics\n",
    "    df[\"topic_label\"] = [\n",
    "        \", \".join([w for (w, _) in model.get_topic(t)[:10]]) if t != -1 else \"Outlier\"\n",
    "        for t in topics\n",
    "    ]\n",
    "    df[\"topic_prob\"] = [\n",
    "        float(np.nanmax(p)) if p is not None and len(p) > 0 else np.nan\n",
    "        for p in probs\n",
    "    ]\n",
    "\n",
    "    df[\"newspaper\"] = newspaper\n",
    "    df[\"year\"] = year\n",
    "\n",
    "    emb_dim = emb.shape[1]\n",
    "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
    "    emb_df = pd.DataFrame(emb, columns=emb_cols).reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df_with_emb = pd.concat([df, emb_df], axis=1)\n",
    "\n",
    "    save_prefix = os.path.splitext(fname)[0]\n",
    "    df_with_emb.to_csv(os.path.join(EMB_DIR, f\"{save_prefix}_doc_topics_with_emb.csv\"), index=False)\n",
    "    model.get_topic_info().to_csv(os.path.join(DATA_DIR, f\"{save_prefix}_topic_summary.csv\"), index=False)\n",
    "\n",
    "    del df, model, emb, df_with_emb\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(glob.glob(os.path.join(DATA_DIR, \"*_gpt.csv\")))\n",
    "    for f in files:\n",
    "        try:\n",
    "            process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f}: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e1cc4",
   "metadata": {},
   "source": [
    "### 일본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854d1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "\n",
      "===== Processing (영문번역 추가)_아사히신문_2022년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_아사히신문_2022년도 데이터.csv | Topics: 2 | Score: 0.9301\n",
      "\n",
      "===== Processing (영문번역 추가)_아사히신문_2023년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_아사히신문_2023년도 데이터.csv | Topics: 3 | Score: 0.9307\n",
      "\n",
      "===== Processing (영문번역 추가)_요미우리신문_2022년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_요미우리신문_2022년도 데이터.csv | Topics: 3 | Score: 0.9316\n",
      "\n",
      "===== Processing (영문번역 추가)_요미우리신문_2024년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_요미우리신문_2024년도 데이터.csv | Topics: 4 | Score: 0.9175\n",
      "\n",
      "===== Processing 20251112_중복기사 삭제완료_(영문번역 추가)_아사히신문_2024년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 20251112_중복기사 삭제완료_(영문번역 추가)_아사히신문_2024년도 데이터.csv | Topics: 4 | Score: 0.9327\n",
      "\n",
      "===== Processing 20251112_중복기사 삭제완료_(영문번역 추가)_요미우리신문_2023년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 20251112_중복기사 삭제완료_(영문번역 추가)_요미우리신문_2023년도 데이터.csv | Topics: 4 | Score: 0.9334\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import glob\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sentence-transformers / BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "BASE_DIR = \"E:/Data_for_Practice/JapMedia/\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data/jap_data/translated\")\n",
    "EMB_DIR = os.path.join(BASE_DIR, \"data/embed_figure\")\n",
    "TEXT_COL = \"영문 번역\"\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SEED = 42\n",
    "EMBEDDING_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "MIN_TOKENS_AFTER_CLEAN = 10\n",
    "NR_TOPICS_FIXED = 5\n",
    "\n",
    "MIN_DF_FIXED = 0.01\n",
    "MAX_DF_FIXED = 0.90\n",
    "\n",
    "# ---------- Simple Stopwords ----------\n",
    "NEWS_STOPWORDS_EXTRA = {\n",
    "    \"said\",\"will\",\"one\",\"two\",\"new\",\"year\",\"years\",\"percent\",\"also\",\"could\",\"would\",\n",
    "    \"mr\",\"ms\",\"u\",\"korea\",\"south\",\"seoul\",\"korean\",\"however\",\"among\",\"may\",\"many\",\n",
    "    \"made\",\"make\",\"like\",\"since\",\"according\",\"including\",\"told\",\"say\",\"says\",\n",
    "    \"first\",\"last\",\"day\",\"days\",\"week\",\"weeks\",\"month\",\"months\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"time\",\"times\",\"report\",\"reports\",\"reported\",\"yonhap\",\"reuters\",\"ap\",\n",
    "    \"people\",\"company\",\"companies\",\"government\",\"official\",\"officials\",\n",
    "    \"article\",\"news\",\"daily\",\"kim\",\"park\",\"lee\",\"cho\",\"jang\",\"chung\",\"moon\",\"yoon\"\n",
    "}\n",
    "all_stop_words = set(ENGLISH_STOP_WORDS).union(NEWS_STOPWORDS_EXTRA)\n",
    "\n",
    "# ---------- Device ----------\n",
    "def pick_device_for_st():\n",
    "    try:\n",
    "        import torch\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = pick_device_for_st()\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "# ---------- Preprocessing (spacy lemmatizer) ----------\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "except:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def clean_text_lemma(t: str, stopwords: set) -> str:\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" \", t)\n",
    "    t = re.sub(r\"\\b\\d+\\b\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z\\s\\-]\", \" \", t)\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    doc = nlp(t)\n",
    "    lemmas = [token.lemma_ for token in doc\n",
    "              if len(token.lemma_) > 2 and token.lemma_ not in stopwords and token.is_alpha]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# ---------- Embedding Model ----------\n",
    "embedding_model = SentenceTransformer(EMBEDDING_NAME, device=DEVICE)\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=48 if DEVICE == \"cpu\" else 128,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "# ---------- Topic Model Scoring ----------\n",
    "def topic_diversity(topic_model, topn=10):\n",
    "    info = topic_model.get_topic_info()\n",
    "    topic_ids = [t for t in info[\"Topic\"] if t != -1]\n",
    "    words = []\n",
    "    for t in topic_ids:\n",
    "        reps = topic_model.get_topic(t)\n",
    "        if reps:\n",
    "            words.extend([w for (w, _) in reps[:topn]])\n",
    "    return len(set(words)) / (len(topic_ids) * topn) if len(topic_ids) else 0\n",
    "\n",
    "def topic_cohesion(embeddings, labels):\n",
    "    labels = np.array(labels)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() < 2:\n",
    "        return 0.0\n",
    "    emb = embeddings[mask]\n",
    "    lab = labels[mask]\n",
    "    sims = []\n",
    "    for t in np.unique(lab):\n",
    "        idx = np.where(lab == t)[0]\n",
    "        if len(idx) > 1:\n",
    "            sub = emb[idx]\n",
    "            centroid = np.mean(sub, axis=0, keepdims=True)\n",
    "            sims.extend(cosine_similarity(sub, centroid).ravel())\n",
    "    return np.mean(sims) if sims else 0.0\n",
    "\n",
    "def composite_score(model, emb, labels):\n",
    "    td = topic_diversity(model)\n",
    "    coh = topic_cohesion(emb, labels)\n",
    "    return 0.6 * td + 0.4 * coh, td, coh\n",
    "\n",
    "# ---------- Fit-Evaluate ----------\n",
    "def fit_evaluate(params, docs, emb):\n",
    "    if len(docs) < 5:\n",
    "        return -9999, 0, 0, 0, None\n",
    "\n",
    "    vec = CountVectorizer(\n",
    "        stop_words=list(all_stop_words),\n",
    "        ngram_range=params[\"n_gram_range\"],\n",
    "        min_df=MIN_DF_FIXED,\n",
    "        max_df=MAX_DF_FIXED\n",
    "    )\n",
    "\n",
    "    umap_m = UMAP(\n",
    "        n_neighbors=params[\"n_neighbors\"],\n",
    "        min_dist=params[\"min_dist\"],\n",
    "        n_components=10,\n",
    "        metric=\"cosine\",\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    hdb = HDBSCAN(\n",
    "        min_cluster_size=params[\"min_cluster_size\"],\n",
    "        min_samples=5,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    ctfidf = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "    model = BERTopic(\n",
    "        n_gram_range=params[\"n_gram_range\"],\n",
    "        nr_topics=NR_TOPICS_FIXED,\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vec,\n",
    "        ctfidf_model=ctfidf,\n",
    "        umap_model=umap_m,\n",
    "        hdbscan_model=hdb,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    topics, _ = model.fit_transform(docs, emb)\n",
    "    score, td, coh = composite_score(model, emb, topics)\n",
    "    n_topics = len([t for t in model.get_topic_info()[\"Topic\"] if t != -1])\n",
    "\n",
    "    return score, td, coh, n_topics, model\n",
    "\n",
    "\n",
    "# ---------- File Processor ----------\n",
    "import re\n",
    "\n",
    "def process_file(file_path: str):\n",
    "    fname = os.path.basename(file_path)\n",
    "    print(f\"\\n===== Processing {fname} =====\")\n",
    "\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    if TEXT_COL not in df.columns:\n",
    "        print(f\"[SKIP] No '{TEXT_COL}' column\")\n",
    "        return\n",
    "\n",
    "    # ---------- 파일명에서 신문사 / 연도 파싱 ----------\n",
    "    # 예: \"(영문번역 추가)_아사히신문_2022년도 데이터.csv\"\n",
    "    base_no_ext = os.path.splitext(fname)[0]\n",
    "\n",
    "    # 신문사 이름: 첫 번째 언더스코어 이후, 두 번째 언더스코어 이전\n",
    "    # (영문번역 추가)_아사히신문_2022년도 데이터  -> \"아사히신문\"\n",
    "    m_np = re.search(r\"_([^_]+)_\\d{4}년도\", base_no_ext)\n",
    "    if m_np:\n",
    "        newspaper = m_np.group(1)\n",
    "    else:\n",
    "        newspaper = \"UNKNOWN\"\n",
    "        print(f\"[WARN] Could not parse newspaper name from filename: {fname}\")\n",
    "\n",
    "    # 연도: \"2022년도\", \"2023년도\" 등에서 4자리 숫자만 추출\n",
    "    m_year = re.search(r\"_(\\d{4})년도\", base_no_ext)\n",
    "    if m_year:\n",
    "        year = int(m_year.group(1))\n",
    "    else:\n",
    "        year = None\n",
    "        print(f\"[WARN] Could not parse year from filename: {fname}\")\n",
    "\n",
    "    # ---------- 텍스트 전처리 ----------\n",
    "    df = df[df[TEXT_COL].notna()].copy()\n",
    "    df[\"clean_text\"] = df[TEXT_COL].map(lambda x: clean_text_lemma(x, all_stop_words))\n",
    "    df = df[df[\"clean_text\"].str.split().map(len) >= MIN_TOKENS_AFTER_CLEAN]\n",
    "    docs = df[\"clean_text\"].tolist()\n",
    "    if len(docs) < 5:\n",
    "        print(f\"[SKIP] Too few docs ({len(docs)})\")\n",
    "        return\n",
    "\n",
    "    # ---------- 문서 임베딩 ----------\n",
    "    emb = encode_texts(docs)   # shape: (n_docs, d)\n",
    "\n",
    "    # ---------- 토픽 모델링 ----------\n",
    "    best_param = {\"n_gram_range\": (1,1), \"n_neighbors\": 10, \"min_dist\": 0.1, \"min_cluster_size\": 25}\n",
    "    score, td, coh, n_topics, model = fit_evaluate(best_param, docs, emb)\n",
    "    print(f\"[DONE] {fname} | Topics: {n_topics} | Score: {score:.4f}\")\n",
    "\n",
    "    topics, probs = model.fit_transform(docs, emb)\n",
    "\n",
    "    df[\"topic\"] = topics\n",
    "    df[\"topic_label\"] = [\n",
    "        \", \".join([w for (w, _) in model.get_topic(t)[:10]]) if t != -1 else \"Outlier\"\n",
    "        for t in topics\n",
    "    ]\n",
    "    df[\"topic_prob\"] = [\n",
    "        float(np.nanmax(p)) if p is not None and len(p) > 0 else np.nan\n",
    "        for p in probs\n",
    "    ]\n",
    "\n",
    "    # ---------- 메타데이터 추가 ----------\n",
    "    df[\"newspaper\"] = newspaper\n",
    "    df[\"year\"] = year\n",
    "\n",
    "    # ---------- 임베딩을 컬럼으로 붙이기 ----------\n",
    "    emb_dim = emb.shape[1]\n",
    "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
    "    emb_df = pd.DataFrame(emb, columns=emb_cols).reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df_with_emb = pd.concat([df, emb_df], axis=1)\n",
    "\n",
    "    # ---------- 저장 ----------\n",
    "    base_for_save = os.path.splitext(fname)[0]   # 확장자 제거\n",
    "    topic_info = model.get_topic_info()\n",
    "    topic_info.to_csv(os.path.join(DATA_DIR, f\"{base_for_save}_topic_summary.csv\"), index=False)\n",
    "\n",
    "    df_with_emb.to_csv(os.path.join(EMB_DIR, f\"{base_for_save}_doc_topics_with_emb.csv\"), index=False)\n",
    "\n",
    "    # 메모리 정리\n",
    "    del df, emb, model, df_with_emb\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(glob.glob(os.path.join(DATA_DIR, \"*데이터.csv\")))\n",
    "    for f in files:\n",
    "        try:\n",
    "            process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] in {f}: {e}\")\n",
    "            traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
