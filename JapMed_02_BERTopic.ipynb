{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f89267",
   "metadata": {},
   "source": [
    "### 한일 기후변화비교 연구\n",
    "#### Step 2. 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00808d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "\n",
      "===== Processing 조선일보_2022년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 조선일보_2022년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9276\n",
      "\n",
      "===== Processing 조선일보_2023년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 조선일보_2023년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9148\n",
      "\n",
      "===== Processing 조선일보_2024년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 조선일보_2024년도 데이터_translated_gpt.csv | Topics: 3 | Score: 0.9272\n",
      "\n",
      "===== Processing 한겨레_2022년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6/6 [00:04<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 한겨레_2022년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9298\n",
      "\n",
      "===== Processing 한겨레_2023년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 한겨레_2023년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.8858\n",
      "\n",
      "===== Processing 한겨레_2024년도 데이터_translated_gpt.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 한겨레_2024년도 데이터_translated_gpt.csv | Topics: 4 | Score: 0.9313\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "BASE_DIR = \"E:/Data_for_Practice/JapMedia/\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data/kor_data\")\n",
    "TEXT_COL = \"본문_en\"\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SEED = 42\n",
    "EMBEDDING_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "MIN_TOKENS_AFTER_CLEAN = 10\n",
    "NR_TOPICS_FIXED = 5\n",
    "\n",
    "MIN_DF_FIXED = 0.01\n",
    "MAX_DF_FIXED = 0.90\n",
    "\n",
    "TOPN_WORDS_FOR_DIVERSITY = 10\n",
    "ALPHA_DIVERSITY = 0.6\n",
    "BETA_COHESION = 0.4\n",
    "\n",
    "ALLOWED_NGRAM = [(1,1), (1,2)]\n",
    "ALLOWED_N_NEIGHBORS = [10, 15]\n",
    "ALLOWED_MIN_DIST    = [0.1, 0.2]\n",
    "ALLOWED_MIN_CLUSTER = [15, 25, 35]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- Device ----------\n",
    "def pick_device_for_st():\n",
    "    try:\n",
    "        import torch\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = pick_device_for_st()\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "# ---------- Stopwords ----------\n",
    "NEWS_STOPWORDS_EXTRA = {\n",
    "    \"said\",\"will\",\"one\",\"two\",\"new\",\"year\",\"years\",\"percent\",\"also\",\"could\",\"would\",\n",
    "    \"mr\",\"ms\",\"u\",\"korea\",\"south\",\"seoul\",\"korean\",\"however\",\"among\",\"may\",\"many\",\n",
    "    \"made\",\"make\",\"like\",\"since\",\"according\",\"including\",\"told\",\"say\",\"says\",\n",
    "    \"first\",\"last\",\"day\",\"days\",\"week\",\"weeks\",\"month\",\"months\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"time\",\"times\",\"report\",\"reports\",\"reported\",\"yonhap\",\"reuters\",\"ap\",\n",
    "    \"people\",\"company\",\"companies\",\"government\",\"official\",\"officials\",\n",
    "    \"article\",\"news\",\"daily\",\"kim\",\"park\",\"lee\",\"cho\",\"jang\",\"chung\",\"moon\",\"yoon\"\n",
    "}\n",
    "all_stop_words = set(ENGLISH_STOP_WORDS).union(NEWS_STOPWORDS_EXTRA)\n",
    "\n",
    "# ---------- Preprocessing ----------\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "except Exception:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def clean_text_lemma(t: str, stopwords: set) -> str:\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" \", t)\n",
    "    t = re.sub(r\"\\b\\d+\\b\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z\\s\\-]\", \" \", t)\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    doc = nlp(t)\n",
    "    lemmas = [\n",
    "        token.lemma_ for token in doc\n",
    "        if len(token.lemma_) > 2 and token.lemma_ not in stopwords and token.is_alpha\n",
    "    ]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# ---------- Embedding ----------\n",
    "embedding_model = SentenceTransformer(EMBEDDING_NAME, device=DEVICE)\n",
    "def encode_texts(texts):\n",
    "    return embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=48 if DEVICE == \"cpu\" else 128,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "# ---------- Scoring ----------\n",
    "def topic_diversity(topic_model, topn=TOPN_WORDS_FOR_DIVERSITY):\n",
    "    info = topic_model.get_topic_info()\n",
    "    topic_ids = [t for t in info[\"Topic\"].tolist() if t != -1]\n",
    "    words = []\n",
    "    for t in topic_ids:\n",
    "        reps = topic_model.get_topic(t)\n",
    "        if reps:\n",
    "            words.extend([w for (w, _) in reps[:topn]])\n",
    "    return len(set(words)) / (len(set(topic_ids)) * topn) if topic_ids else 0.0\n",
    "\n",
    "def topic_cohesion(embeddings, labels):\n",
    "    labels = np.array(labels)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() < 2:\n",
    "        return 0.0\n",
    "    emb = embeddings[mask]\n",
    "    lab = labels[mask]\n",
    "    sims = []\n",
    "    for t in np.unique(lab):\n",
    "        idx = np.where(lab == t)[0]\n",
    "        if len(idx) > 1:\n",
    "            sub = emb[idx]\n",
    "            centroid = np.mean(sub, axis=0, keepdims=True)\n",
    "            sims.extend(cosine_similarity(sub, centroid).ravel())\n",
    "    return np.mean(sims) if sims else 0.0\n",
    "\n",
    "def composite_score(model, emb, labels, alpha=ALPHA_DIVERSITY, beta=BETA_COHESION):\n",
    "    td = topic_diversity(model)\n",
    "    coh = topic_cohesion(emb, labels)\n",
    "    return alpha * td + beta * coh, td, coh\n",
    "\n",
    "# ---------- Fit function ----------\n",
    "def fit_evaluate(params, docs, emb):\n",
    "    if len(docs) < 5: return -9999, 0, 0, 0, None\n",
    "    vec = CountVectorizer(stop_words=list(all_stop_words),\n",
    "                          ngram_range=params[\"n_gram_range\"],\n",
    "                          min_df=MIN_DF_FIXED, max_df=MAX_DF_FIXED)\n",
    "    umap_m = UMAP(n_neighbors=params[\"n_neighbors\"], min_dist=params[\"min_dist\"],\n",
    "                  n_components=10, metric=\"cosine\", random_state=SEED)\n",
    "    hdb = HDBSCAN(min_cluster_size=params[\"min_cluster_size\"], min_samples=5,\n",
    "                  metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n",
    "    ctfidf = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    model = BERTopic(n_gram_range=params[\"n_gram_range\"],\n",
    "                     nr_topics=NR_TOPICS_FIXED,\n",
    "                     embedding_model=embedding_model,\n",
    "                     vectorizer_model=vec, ctfidf_model=ctfidf,\n",
    "                     umap_model=umap_m, hdbscan_model=hdb,\n",
    "                     calculate_probabilities=True, verbose=False)\n",
    "    topics, _ = model.fit_transform(docs, emb)\n",
    "    score, td, coh = composite_score(model, emb, topics)\n",
    "    n_topics = len([t for t in model.get_topic_info()[\"Topic\"] if t != -1])\n",
    "    return score, td, coh, n_topics, model\n",
    "\n",
    "# ---------- Per-file pipeline ----------\n",
    "def process_file(file_path: str):\n",
    "    fname = os.path.basename(file_path)\n",
    "    print(f\"\\n===== Processing {fname} =====\")\n",
    "\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    if TEXT_COL not in df.columns:\n",
    "        print(f\"[SKIP] No '{TEXT_COL}' column\")\n",
    "        return\n",
    "    df = df[df[TEXT_COL].notna()].copy()\n",
    "    df[\"clean_text\"] = df[TEXT_COL].map(lambda x: clean_text_lemma(x, all_stop_words))\n",
    "    df = df[df[\"clean_text\"].str.split().map(len) >= MIN_TOKENS_AFTER_CLEAN]\n",
    "    docs = df[\"clean_text\"].tolist()\n",
    "    if len(docs) < 5:\n",
    "        print(f\"[SKIP] Too few docs ({len(docs)})\")\n",
    "        return\n",
    "\n",
    "    emb = encode_texts(docs)\n",
    "\n",
    "    # Best param (고정된 단일 조합 사용)\n",
    "    best_param = {\"n_gram_range\": (1,1), \"n_neighbors\": 10, \"min_dist\": 0.1, \"min_cluster_size\": 25}\n",
    "    score, td, coh, n_topics, model = fit_evaluate(best_param, docs, emb)\n",
    "    print(f\"[DONE] {fname} | Topics: {n_topics} | Score: {score:.4f}\")\n",
    "\n",
    "    topics, probs = model.fit_transform(docs, emb)\n",
    "\n",
    "    df[\"topic\"] = topics\n",
    "    df[\"topic_label\"] = [\", \".join([w for (w, _) in model.get_topic(t)[:10]]) if t != -1 else \"Outlier\" for t in topics]\n",
    "    df[\"topic_prob\"] = [float(np.nanmax(p)) if p is not None and len(p)>0 else np.nan for p in probs]\n",
    "\n",
    "    topic_info = model.get_topic_info()\n",
    "    topic_info.to_csv(os.path.join(DATA_DIR, f\"{fname.replace('.csv', '')}_topic_summary.csv\"), index=False)\n",
    "    df.to_csv(os.path.join(DATA_DIR, f\"{fname.replace('.csv', '')}_doc_topics.csv\"), index=False)\n",
    "\n",
    "    # ----------- 아래는 주석 처리된 부분 -----------\n",
    "    # # (1) Hyperparam trials 저장\n",
    "    # stage_df.to_csv(os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_hp_trials.csv\"), index=False)\n",
    "    #\n",
    "    # # (2) 시각화 HTML 저장\n",
    "    # fig_bar = model.visualize_barchart(top_n_topics=NR_TOPICS_FIXED)\n",
    "    # fig_bar.write_html(os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_viz_barchart.html\"))\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    del df, emb, model\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(glob.glob(os.path.join(DATA_DIR, \"*_gpt.csv\")))\n",
    "    for f in files:\n",
    "        try:\n",
    "            process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f}: {e}\")\n",
    "            traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e1cc4",
   "metadata": {},
   "source": [
    "### 일본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2e32a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "\n",
      "===== Processing (영문번역 추가)_아사히신문_2022년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_아사히신문_2022년도 데이터.csv | Topics: 2 | Score: 0.9301\n",
      "\n",
      "===== Processing (영문번역 추가)_아사히신문_2023년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_아사히신문_2023년도 데이터.csv | Topics: 3 | Score: 0.9307\n",
      "\n",
      "===== Processing (영문번역 추가)_아사히신문_2024년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_아사히신문_2024년도 데이터.csv | Topics: 4 | Score: 0.9327\n",
      "\n",
      "===== Processing (영문번역 추가)_요미우리신문_2022년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_요미우리신문_2022년도 데이터.csv | Topics: 3 | Score: 0.9316\n",
      "\n",
      "===== Processing (영문번역 추가)_요미우리신문_2023년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_요미우리신문_2023년도 데이터.csv | Topics: 4 | Score: 0.9334\n",
      "\n",
      "===== Processing (영문번역 추가)_요미우리신문_2024년도 데이터.csv =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] (영문번역 추가)_요미우리신문_2024년도 데이터.csv | Topics: 4 | Score: 0.9175\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "BASE_DIR = \"E:/Data_for_Practice/JapMedia/\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data/jap_data/translated\")\n",
    "TEXT_COL = \"영문 번역\"\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SEED = 42\n",
    "EMBEDDING_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "MIN_TOKENS_AFTER_CLEAN = 10\n",
    "NR_TOPICS_FIXED = 5\n",
    "\n",
    "MIN_DF_FIXED = 0.01\n",
    "MAX_DF_FIXED = 0.90\n",
    "\n",
    "TOPN_WORDS_FOR_DIVERSITY = 10\n",
    "ALPHA_DIVERSITY = 0.6\n",
    "BETA_COHESION = 0.4\n",
    "\n",
    "ALLOWED_NGRAM = [(1,1), (1,2)]\n",
    "ALLOWED_N_NEIGHBORS = [10, 15]\n",
    "ALLOWED_MIN_DIST    = [0.1, 0.2]\n",
    "ALLOWED_MIN_CLUSTER = [15, 25, 35]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- Device ----------\n",
    "def pick_device_for_st():\n",
    "    try:\n",
    "        import torch\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = pick_device_for_st()\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "# ---------- Stopwords ----------\n",
    "NEWS_STOPWORDS_EXTRA = {\n",
    "    \"said\",\"will\",\"one\",\"two\",\"new\",\"year\",\"years\",\"percent\",\"also\",\"could\",\"would\",\n",
    "    \"mr\",\"ms\",\"u\",\"korea\",\"south\",\"seoul\",\"korean\",\"however\",\"among\",\"may\",\"many\",\n",
    "    \"made\",\"make\",\"like\",\"since\",\"according\",\"including\",\"told\",\"say\",\"says\",\n",
    "    \"first\",\"last\",\"day\",\"days\",\"week\",\"weeks\",\"month\",\"months\",\"today\",\"yesterday\",\"tomorrow\",\n",
    "    \"time\",\"times\",\"report\",\"reports\",\"reported\",\"yonhap\",\"reuters\",\"ap\",\n",
    "    \"people\",\"company\",\"companies\",\"government\",\"official\",\"officials\",\n",
    "    \"article\",\"news\",\"daily\",\"kim\",\"park\",\"lee\",\"cho\",\"jang\",\"chung\",\"moon\",\"yoon\"\n",
    "}\n",
    "all_stop_words = set(ENGLISH_STOP_WORDS).union(NEWS_STOPWORDS_EXTRA)\n",
    "\n",
    "# ---------- Preprocessing ----------\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "except Exception:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def clean_text_lemma(t: str, stopwords: set) -> str:\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" \", t)\n",
    "    t = re.sub(r\"\\b\\d+\\b\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z\\s\\-]\", \" \", t)\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    doc = nlp(t)\n",
    "    lemmas = [\n",
    "        token.lemma_ for token in doc\n",
    "        if len(token.lemma_) > 2 and token.lemma_ not in stopwords and token.is_alpha\n",
    "    ]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# ---------- Embedding ----------\n",
    "embedding_model = SentenceTransformer(EMBEDDING_NAME, device=DEVICE)\n",
    "def encode_texts(texts):\n",
    "    return embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=48 if DEVICE == \"cpu\" else 128,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "# ---------- Scoring ----------\n",
    "def topic_diversity(topic_model, topn=TOPN_WORDS_FOR_DIVERSITY):\n",
    "    info = topic_model.get_topic_info()\n",
    "    topic_ids = [t for t in info[\"Topic\"].tolist() if t != -1]\n",
    "    words = []\n",
    "    for t in topic_ids:\n",
    "        reps = topic_model.get_topic(t)\n",
    "        if reps:\n",
    "            words.extend([w for (w, _) in reps[:topn]])\n",
    "    return len(set(words)) / (len(set(topic_ids)) * topn) if topic_ids else 0.0\n",
    "\n",
    "def topic_cohesion(embeddings, labels):\n",
    "    labels = np.array(labels)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() < 2:\n",
    "        return 0.0\n",
    "    emb = embeddings[mask]\n",
    "    lab = labels[mask]\n",
    "    sims = []\n",
    "    for t in np.unique(lab):\n",
    "        idx = np.where(lab == t)[0]\n",
    "        if len(idx) > 1:\n",
    "            sub = emb[idx]\n",
    "            centroid = np.mean(sub, axis=0, keepdims=True)\n",
    "            sims.extend(cosine_similarity(sub, centroid).ravel())\n",
    "    return np.mean(sims) if sims else 0.0\n",
    "\n",
    "def composite_score(model, emb, labels, alpha=ALPHA_DIVERSITY, beta=BETA_COHESION):\n",
    "    td = topic_diversity(model)\n",
    "    coh = topic_cohesion(emb, labels)\n",
    "    return alpha * td + beta * coh, td, coh\n",
    "\n",
    "# ---------- Fit function ----------\n",
    "def fit_evaluate(params, docs, emb):\n",
    "    if len(docs) < 5: return -9999, 0, 0, 0, None\n",
    "    vec = CountVectorizer(stop_words=list(all_stop_words),\n",
    "                          ngram_range=params[\"n_gram_range\"],\n",
    "                          min_df=MIN_DF_FIXED, max_df=MAX_DF_FIXED)\n",
    "    umap_m = UMAP(n_neighbors=params[\"n_neighbors\"], min_dist=params[\"min_dist\"],\n",
    "                  n_components=10, metric=\"cosine\", random_state=SEED)\n",
    "    hdb = HDBSCAN(min_cluster_size=params[\"min_cluster_size\"], min_samples=5,\n",
    "                  metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n",
    "    ctfidf = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    model = BERTopic(n_gram_range=params[\"n_gram_range\"],\n",
    "                     nr_topics=NR_TOPICS_FIXED,\n",
    "                     embedding_model=embedding_model,\n",
    "                     vectorizer_model=vec, ctfidf_model=ctfidf,\n",
    "                     umap_model=umap_m, hdbscan_model=hdb,\n",
    "                     calculate_probabilities=True, verbose=False)\n",
    "    topics, _ = model.fit_transform(docs, emb)\n",
    "    score, td, coh = composite_score(model, emb, topics)\n",
    "    n_topics = len([t for t in model.get_topic_info()[\"Topic\"] if t != -1])\n",
    "    return score, td, coh, n_topics, model\n",
    "\n",
    "# ---------- Per-file pipeline ----------\n",
    "def process_file(file_path: str):\n",
    "    fname = os.path.basename(file_path)\n",
    "    print(f\"\\n===== Processing {fname} =====\")\n",
    "\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    if TEXT_COL not in df.columns:\n",
    "        print(f\"[SKIP] No '{TEXT_COL}' column\")\n",
    "        return\n",
    "    df = df[df[TEXT_COL].notna()].copy()\n",
    "    df[\"clean_text\"] = df[TEXT_COL].map(lambda x: clean_text_lemma(x, all_stop_words))\n",
    "    df = df[df[\"clean_text\"].str.split().map(len) >= MIN_TOKENS_AFTER_CLEAN]\n",
    "    docs = df[\"clean_text\"].tolist()\n",
    "    if len(docs) < 5:\n",
    "        print(f\"[SKIP] Too few docs ({len(docs)})\")\n",
    "        return\n",
    "\n",
    "    emb = encode_texts(docs)\n",
    "\n",
    "    # Best param (고정된 단일 조합 사용)\n",
    "    best_param = {\"n_gram_range\": (1,1), \"n_neighbors\": 10, \"min_dist\": 0.1, \"min_cluster_size\": 25}\n",
    "    score, td, coh, n_topics, model = fit_evaluate(best_param, docs, emb)\n",
    "    print(f\"[DONE] {fname} | Topics: {n_topics} | Score: {score:.4f}\")\n",
    "\n",
    "    topics, probs = model.fit_transform(docs, emb)\n",
    "\n",
    "    df[\"topic\"] = topics\n",
    "    df[\"topic_label\"] = [\", \".join([w for (w, _) in model.get_topic(t)[:10]]) if t != -1 else \"Outlier\" for t in topics]\n",
    "    df[\"topic_prob\"] = [float(np.nanmax(p)) if p is not None and len(p)>0 else np.nan for p in probs]\n",
    "\n",
    "    topic_info = model.get_topic_info()\n",
    "    topic_info.to_csv(os.path.join(DATA_DIR, f\"{fname.replace('.csv', '')}_topic_summary.csv\"), index=False)\n",
    "    df.to_csv(os.path.join(DATA_DIR, f\"{fname.replace('.csv', '')}_doc_topics.csv\"), index=False)\n",
    "\n",
    "    # ----------- 아래는 주석 처리된 부분 -----------\n",
    "    # # (1) Hyperparam trials 저장\n",
    "    # stage_df.to_csv(os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_hp_trials.csv\"), index=False)\n",
    "    #\n",
    "    # # (2) 시각화 HTML 저장\n",
    "    # fig_bar = model.visualize_barchart(top_n_topics=NR_TOPICS_FIXED)\n",
    "    # fig_bar.write_html(os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_viz_barchart.html\"))\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    del df, emb, model\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(glob.glob(os.path.join(DATA_DIR, \"*데이터.csv\")))\n",
    "    for f in files:\n",
    "        try:\n",
    "            process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f}: {e}\")\n",
    "            traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx5090",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
